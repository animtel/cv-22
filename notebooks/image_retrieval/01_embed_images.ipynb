{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3a174e-01cb-4af1-98d7-611561effd69",
   "metadata": {},
   "source": [
    "### Notebook description:\n",
    "\n",
    "This notebooks should: \n",
    "1. read images and embed it by CLIP model\n",
    "1. save embeddings in pair with filename to pickle object\n",
    "\n",
    "At the end we will have just embedding collection that we can use to find clusters or to make image retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92680c53-073c-4578-9435-d8b3a70b40c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path.ls = lambda x: list(x.iterdir())\n",
    "\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff8065f3-2cf4-4c18-8f35-e6958cc9630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('data')\n",
    "DIR_IMGS = DATA_DIR / 'drone_imgs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e198bc9-cded-4af5-82f2-df910004925d",
   "metadata": {},
   "source": [
    "# Get image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b7c73d7-be86-462c-af3b-ec9792157598",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/openai/CLIP.git -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c139e540-b89e-4b43-bc71-9a6bea357abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18890c16-5e38-47bd-923b-b0c067a6c67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ImageEmbedding:\n",
    "    \"\"\"Class for keeping track of an item in inventory.\"\"\"\n",
    "    emb: torch.Tensor # clip image embedding: B, 768\n",
    "    filename: Path\n",
    "\n",
    "    def __init__(self, filename: str, emb: torch.Tensor):\n",
    "        self.filename = filename\n",
    "        self.emb = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d583738-52af-428d-8545-57f730864661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "578d584f-4936-4c21-88d0-8c66d2d2fb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "model.eval()\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd77a301-0f4b-4f8a-81f7-0c5b0e7f4c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8201/8201 [03:16<00:00, 41.68it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_images = []\n",
    "for img_file in tqdm(DIR_IMGS.ls()):\n",
    "    if img_file.suffix != '.png':\n",
    "        continue\n",
    "        \n",
    "    image = preprocess(Image.open(img_file)).unsqueeze(0).to(device)\n",
    "    with torch.inference_mode():\n",
    "        image_features = model.encode_image(image)\n",
    "    \n",
    "    img_obj = ImageEmbedding(img_file.name, image_features.cpu())\n",
    "    processed_images.append(img_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24b8f8af-b940-477d-9415-daa7f8793557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8201"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf449b6-af9d-41a5-9234-0da4b000d3e9",
   "metadata": {},
   "source": [
    "# Save processed images to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77aacd7e-f811-4bf5-927a-0e6fcb6c062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def write_pickle(filename, obj):\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(obj, handle)\n",
    "\n",
    "def read_pickle(filename):\n",
    "    with open(filename, 'rb') as handle:\n",
    "        return pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8beabe65-d37a-403d-8b74-ddd1f1910276",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pickle('data/processed_images.pickle', processed_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
